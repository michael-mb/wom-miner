{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tomllib\n",
    "import xml.etree.ElementTree as ET\n",
    "from elasticsearch import Elasticsearch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Elastic connection\n",
    "with open(Path(f\"./config/remote_elastic.toml\"), \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "disable_security = config.get('disable_security', False)\n",
    "es = Elasticsearch(\n",
    "    config['instance'],\n",
    "    basic_auth=(config['username'], config['password']),\n",
    "    verify_certs=not disable_security,\n",
    "    ssl_show_warn=not disable_security\n",
    ")\n",
    "\n",
    "if not es:\n",
    "    raise RuntimeError(\"Could not configure Elasticsearch instance\")\n",
    "if not es.ping():\n",
    "    raise RuntimeError(\"Elasticsearch instance not available\")\n",
    "\n",
    "\n",
    "# Miner Configuration\n",
    "labeled_data = \"classification_train_index-ude\"\n",
    "preprocessing = \"preprocessing-ude\"\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "research_project_index = config['research_project_index']\n",
    "\n",
    "# create index if not exist\n",
    "es.indices.create(index=research_project_index, ignore=400) # ignore 400 Index Already Exists exception\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract Preprocessing data page from Elastic\n",
    "\n",
    "def reformat_content(txt):\n",
    "    txt = txt.replace(\"http\", \"   http\")\n",
    "    txt = txt.replace(\"mailto\", \"   mailto\")\n",
    "    txt = txt.replace(\"](/\",\" \")\n",
    "\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "    # Remove URLs from the text\n",
    "    txt = re.sub(url_pattern, '', txt)\n",
    "\n",
    "    # Regular expression pattern to match PDF filenames\n",
    "    pdf_filename_pattern = re.compile(r'\\b[\\w-]+\\.pdf\\b', re.IGNORECASE)\n",
    "\n",
    "    # Remove PDF filenames from the text\n",
    "    txt = re.sub(pdf_filename_pattern, '', txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "def extract_data_from_labeled_index(elastic, name_input, query, last_sort_index):\n",
    "\n",
    "    if last_sort_index == 0:\n",
    "        search_result = elastic.search(index=name_input, body=query, sort={\"url.keyword\": {\"order\": \"asc\"}} )\n",
    "    else:\n",
    "        search_result = elastic.search(index=name_input, body=query, search_after=last_sort_index, sort={\"url.keyword\": {\"order\": \"asc\"}})\n",
    "\n",
    "    for hit in search_result['hits']['hits']:\n",
    "        extracted_data = {\n",
    "            'url': hit['_source']['url'],\n",
    "            'label': hit['_source']['label']\n",
    "        }\n",
    "\n",
    "        yield extracted_data, hit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def contains_keyword(keywords, text):\n",
    "    for keyword in keywords:\n",
    "           pattern = r\"\\b\" + re.escape(keyword) + r\"\\w*\\b\"\n",
    "           if re.search(pattern, text, flags=re.IGNORECASE):\n",
    "               return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def should_be_ignored(data):\n",
    "\n",
    "    project_keywords = [\"projektbeschreibung\",\"projektleitung\", \"projektkoordination\",\n",
    "                        \"projektfÃ¶rderung\", \"projektlaufzeit\", \"Projektbearbeiter\"\n",
    "                        \"project description\", \"project management\", \"project coordination\",\n",
    "                        \"project funding\", \"project duration\"]\n",
    "\n",
    "    if contains_keyword(project_keywords, data['extracted_xml']):\n",
    "        return False\n",
    "\n",
    "    # SIMULATION ELASTIC REGEXES\n",
    "    elastic_keywords = [\"project\", \"projekt\", \"research\", \"forschung\"]\n",
    "\n",
    "    if not contains_keyword(elastic_keywords, data['url']):\n",
    "        #print(\"IGNORE URL !!!!! -->\", data['url'])\n",
    "        return True\n",
    "\n",
    "\n",
    "    # List of keywords to check\n",
    "    keywords = [\"pdf\", \"prof\", \"download\", \"publication\", \"publikation\", \"archiv\", \"promov\", \"team\", \"talks\",\n",
    "                \"vortraege\", \"associate\", \"Stellenausschreibung\", \"job\"\n",
    "                \"meldungen\", \"office\", \"secretary\",\n",
    "                \"student\", \"hilfskraft\", \"assistenz\", \"mitarbeiter\", \"angebote\", \"forum\", \"elnrw\", \"termin\",\n",
    "                \"neuigkeit\", \"arbeitsgruppe\", \"dr.\", \"research group\", \"working group\", \"theme\", \"coop\", \"koop\"\n",
    "                ]\n",
    "\n",
    "    # In - Text\n",
    "\n",
    "    # In - URL\n",
    "    if contains_keyword(keywords, data['url']):\n",
    "        return True\n",
    "\n",
    "    # In - XML content\n",
    "    root = ET.fromstring(data['extracted_xml'])\n",
    "    h1_elements = root.findall(\".//head[@rend='h1']\")\n",
    "    h2_elements = root.findall(\".//head[@rend='h2']\")\n",
    "\n",
    "    if h1_elements:\n",
    "        for h1_element in h1_elements:\n",
    "            h1_text = h1_element.text.lower()\n",
    "            if contains_keyword(keywords, h1_text):\n",
    "                return True\n",
    "\n",
    "    if h2_elements:\n",
    "        for h2_element in h2_elements:\n",
    "            if h2_element.text is not None:\n",
    "                h2_text = h2_element.text.lower()\n",
    "                if contains_keyword(keywords, h2_text):\n",
    "                   return True\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mining logic\n",
    "\n",
    "query = {\n",
    "        \"query\": {\"match_all\": {}}, \"size\": 100\n",
    "}\n",
    "\n",
    "num_documents = es.count(index=labeled_data)['count']\n",
    "print(\"num_documents:\", num_documents)\n",
    "\n",
    "num_batches = (num_documents + batch_size - 1) // batch_size\n",
    "print(\"num_batches:\",num_batches)\n",
    "\n",
    "\n",
    "#response = es.search(index=labeled_data, body=query)\n",
    "\n",
    "labeled_data_list = []\n",
    "\n",
    "index = 1\n",
    "\n",
    "# Last Sort\n",
    "last_sort = 0\n",
    "\n",
    "# Process the response\n",
    "for batch_number in range(num_batches):\n",
    "    print(\"last_sort\", last_sort)\n",
    "    for data, raw_data  in extract_data_from_labeled_index(es, labeled_data, query, last_sort):\n",
    "        last_sort = raw_data['sort']\n",
    "        #print(\"document_number --> \", index)\n",
    "\n",
    "        research_data = {\n",
    "            'id': raw_data['_id'],\n",
    "            'label': data['label'] == 4,\n",
    "            'url': data['url']\n",
    "        }\n",
    "\n",
    "        labeled_data_list.append(research_data)\n",
    "        #print(\"DATA:\",research_data['url'], \" --> \" , research_data['label'])\n",
    "        index += 1\n",
    "\n",
    "\n",
    "index = 0\n",
    "\n",
    "print(\"LEN:\",len(labeled_data_list))\n",
    "\n",
    "for elem in labeled_data_list:\n",
    "    query = {\n",
    "        \"query\": {\"term\": {\"url\": {\"value\": elem['url']}}},\n",
    "        \"size\": 1\n",
    "    }\n",
    "\n",
    "    response = es.search(index=preprocessing, body=query)\n",
    "    # Process the response\n",
    "    if response['timed_out'] is False:\n",
    "        hits = response['hits']['hits']\n",
    "        #print(\"hits:\", hits)\n",
    "        data = {}\n",
    "        for hit in hits:\n",
    "            data['url'] = hit['_source']['url']\n",
    "            data['extracted_xml'] = hit['_source']['content_xml']\n",
    "            data['title'] = hit['_source']['title']\n",
    "\n",
    "            #print(\"DATA:\", data)\n",
    "            elem[\"script_label\"] = not should_be_ignored(data)\n",
    "            #print(\"SCRIPT LABEL:\", elem[\"script_label\"])\n",
    "            #print(\"LABEL:\", elem[\"label\"])\n",
    "\n",
    "        index +=1\n",
    "        print(\"RESPONSE ==>\", index)\n",
    "    else:\n",
    "        print(\"Request timed out.\")\n",
    "        index +=1\n",
    "        print(\"ERROR ==>\", index)\n",
    "\n",
    "    #if index == 10:\n",
    "    #    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "count_script_label_equal_label = 0\n",
    "count_label = 0\n",
    "total_classified_project = 0\n",
    "count_labelling_true = 0\n",
    "count_script_true = 0\n",
    "\n",
    "# Iterate through the element list\n",
    "for elem in labeled_data_list:\n",
    "    count_label += 1\n",
    "    script_label = elem.get(\"script_label\")\n",
    "    label = elem.get(\"label\")\n",
    "\n",
    "    if label:\n",
    "        total_classified_project += 1\n",
    "\n",
    "    if script_label is not None:\n",
    "\n",
    "        if label:\n",
    "            count_labelling_true += 1\n",
    "            if script_label:\n",
    "                count_script_true += 1\n",
    "        # Increment counters based on the comparison\n",
    "        if script_label == label:\n",
    "            count_script_label_equal_label += 1\n",
    "        else:\n",
    "            print(\"NOT OK: \",elem)\n",
    "            print(\"SCRIPT LABEL\",script_label)\n",
    "            print(\"DENNIS LABEL\",label)\n",
    "\n",
    "\"\"\"\n",
    "# Calculate the ratio\n",
    "\"\"\"\n",
    "\n",
    "total_elements = count_label\n",
    "ratio_equal = count_script_label_equal_label / count_label\n",
    "\n",
    "# Print the counts and ratios\n",
    "print(\"Correct element identified by the script => script_label == label:\", count_script_label_equal_label)\n",
    "print(\"Total:\", count_label)\n",
    "print(\"Ratio of script_label == label:\", ratio_equal)\n",
    "\n",
    "print(\"script_True:\", count_script_true)\n",
    "print(\"label_true:\", count_labelling_true)\n",
    "print(\"Ratio of script_label_true / label_true: \", count_script_true/count_labelling_true)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
