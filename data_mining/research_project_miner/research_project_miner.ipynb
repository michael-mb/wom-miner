{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tomllib\n",
    "import os\n",
    "import time\n",
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from elasticsearch import Elasticsearch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Elastic connection\n",
    "with open(Path(f\"./config/elastic.toml\"), \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "disable_security = config.get('disable_security', False)\n",
    "es = Elasticsearch(\n",
    "    config['instance'],\n",
    "    basic_auth=(config['username'], config['password']),\n",
    "    verify_certs=not disable_security,\n",
    "    ssl_show_warn=not disable_security\n",
    ")\n",
    "\n",
    "if not es:\n",
    "    raise RuntimeError(\"Could not configure Elasticsearch instance\")\n",
    "if not es.ping():\n",
    "    raise RuntimeError(\"Elasticsearch instance not available\")\n",
    "\n",
    "# Miner Configuration\n",
    "index_pre = config['pre_index']\n",
    "batch_size = config['batch_size']\n",
    "research_project_index = config['research_project_index']\n",
    "\n",
    "# create index if not exist\n",
    "es.indices.create(index=research_project_index, ignore=400) # ignore 400 Index Already Exists exception"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bart summarization\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        [text],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "def generate_summary(inputs):\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        max_length=1024,\n",
    "        min_length=200,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def summarize(input_text):\n",
    "    start_time = time.time()\n",
    "\n",
    "    inputs = preprocess_text(input_text)\n",
    "    summary = generate_summary(inputs)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Summary time elapsed: \", end_time - start_time)\n",
    "\n",
    "    return  summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# People Recognition\n",
    "english_nlp = spacy.load(\"en_core_web_lg\")\n",
    "german_nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "def extract_people_names(text, nlp):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    people_names = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"PERSON\" or entity.label_ == \"PER\":\n",
    "            people_names.append(entity.text)\n",
    "\n",
    "    return are_proper_nouns(people_names, nlp)\n",
    "\n",
    "def are_proper_nouns(words, nlp):\n",
    "    doc = nlp(\" \".join(words))\n",
    "\n",
    "    proper_nouns = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if len(token.text) > 4:\n",
    "                proper_nouns.add(token.text)\n",
    "\n",
    "    return proper_nouns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract Preprocessing data page from Elastic\n",
    "\n",
    "def reformat_content(txt):\n",
    "    txt = txt.replace(\"http\", \"   http\")\n",
    "    txt = txt.replace(\"mailto\", \"   mailto\")\n",
    "    txt = txt.replace(\"](/\",\" \")\n",
    "\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "    # Remove URLs from the text\n",
    "    txt = re.sub(url_pattern, '', txt)\n",
    "\n",
    "    # Regular expression pattern to match PDF filenames\n",
    "    pdf_filename_pattern = re.compile(r'\\b[\\w-]+\\.pdf\\b', re.IGNORECASE)\n",
    "\n",
    "    # Remove PDF filenames from the text\n",
    "    txt = re.sub(pdf_filename_pattern, '', txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "def extract_data_from_index(elastic, name_input, query, last_sort_index):\n",
    "\n",
    "    if last_sort_index == 0:\n",
    "        search_result = elastic.search(index=name_input, body=query, sort=[{\"timestamp\":\"asc\"}] )\n",
    "    else:\n",
    "        search_result = elastic.search(index=name_input, body=query, search_after=last_sort_index, sort=[{\"timestamp\":\"asc\"}] )\n",
    "\n",
    "    for hit in search_result['hits']['hits']:\n",
    "        extracted_data = {\n",
    "            'url': hit['_source']['url'],\n",
    "            'title': hit['_source']['title'],\n",
    "            'content': reformat_content(hit['_source']['content']),# content_txt\n",
    "            'language': hit['_source']['language'],\n",
    "            'extracted_from': hit['_source']['extracted_from'],\n",
    "            'extracted_xml': hit['_source']['extracted_xml'],\n",
    "            #'person_names': hit['_source']['person_names'],\n",
    "        }\n",
    "\n",
    "        yield extracted_data, hit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def contains_keyword(keywords, text):\n",
    "    for keyword in keywords:\n",
    "           pattern = r\"\\b\" + re.escape(keyword) + r\"\\w*\\b\"\n",
    "           if re.search(pattern, text, flags=re.IGNORECASE):\n",
    "               return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def should_be_ignored(data):\n",
    "    # 1 - First ignore publications\n",
    "\n",
    "    # List of keywords to check\n",
    "    keywords = [\"pdf\", \"prof\", \"download\", \"publication\", \"publikation\", \"archiv\", \"promov\", \"team\", \"talks\",\n",
    "                \"vortraege\", \"associate\",\n",
    "                \"meldungen\", \"office\", \"secretary\",\n",
    "                \"student\", \"hilfskraft\", \"assistenz\", \"mitarbeiter\", \"angebote\", \"forum\", \"elnrw\", \"termin\",\n",
    "                \"neuigkeit\", \"arbeitsgruppe\", \"dr.\", \"research group\", \"working group\", \"theme\", \"coop\", \"koop\"]\n",
    "\n",
    "    # In - URL\n",
    "    if contains_keyword(keywords, data['url']):\n",
    "        return True\n",
    "\n",
    "    # In - XML content\n",
    "    root = ET.fromstring(data['extracted_xml'])\n",
    "    h1_elements = root.findall(\".//head[@rend='h1']\")\n",
    "    h2_elements = root.findall(\".//head[@rend='h2']\")\n",
    "\n",
    "    if h1_elements:\n",
    "        for h1_element in h1_elements:\n",
    "            h1_text = h1_element.text.lower()\n",
    "            if contains_keyword(keywords, h1_text):\n",
    "                return True\n",
    "\n",
    "    if h2_elements:\n",
    "        for h2_element in h2_elements:\n",
    "            if h2_element.text is not None:\n",
    "                h2_text = h2_element.text.lower()\n",
    "                if contains_keyword(keywords, h2_text):\n",
    "                   return True\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mining logic\n",
    "# Get Document with *project* *projekt* *research* or *forschung* in title or in url\n",
    "search_query = {\n",
    "    \"query\": { \"bool\": {\n",
    "            \"should\": [ { \"regexp\": { \"title\": \".*project.*\" } }, { \"regexp\": { \"title\": \".*projekt.*\" } },\n",
    "                        { \"regexp\": { \"title\": \".*research.*\" } }, { \"regexp\": { \"title\": \".*forschung.*\" } },\n",
    "                        { \"regexp\": { \"url\": \".*project.*\" } }, { \"regexp\": { \"url\": \".*projekt.*\" } },\n",
    "                        { \"regexp\": { \"url\": \".*research.*\" } }, { \"regexp\": { \"url\": \".*forschung.*\" } },\n",
    "\n",
    "            #            To retrieve documents that do not necessarily have project in their url or title\n",
    "            #            { \"regexp\": { \"content_xml\": \".*projektbeschreibung.*\" } }, { \"regexp\": { \"content_xml\": \".*projektleitung.*\" } },\n",
    "            #            { \"regexp\": { \"content_xml\": \".*projektkoordination.*\" } }, { \"regexp\": { \"content_xml\": \".*projektförderung.*\" } },\n",
    "\n",
    "            #            { \"regexp\": { \"content_xml\": \".*projektlaufzeit.*\" } }, { \"regexp\": { \"content_xml\": \".*Projektbearbeiter.*\" } },\n",
    "            #            { \"regexp\": { \"content_xml\": \".*project description.*\" } }, { \"regexp\": { \"content_xml\": \".*project management.*\" } },\n",
    "\n",
    "            #            { \"regexp\": { \"content_xml\": \".*project coordination.*\" } }, { \"regexp\": { \"content_xml\": \".*project funding.*\" } },\n",
    "            #            { \"regexp\": { \"content_xml\": \".*project duration.*\" } }\n",
    "                     ],\n",
    "        }\n",
    "    },\n",
    "    \"fields\": [ \"title\", \"url\", \"content_xml\" ]\n",
    "}\n",
    "\n",
    "# Get the number of documents in the index\n",
    "num_documents = es.count(index=index_pre)['count']\n",
    "print(\"num_documents:\",num_documents)\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = (num_documents + batch_size - 1) // batch_size\n",
    "print(\"num_batches:\",num_batches)\n",
    "\n",
    "# Last Sort\n",
    "last_sort = 0\n",
    "\n",
    "# Only for test: limit definition\n",
    "index = 1\n",
    "limit = 100\n",
    "break_outer_loop = False\n",
    "\n",
    "ignored_page = 0\n",
    "considered_page = 0\n",
    "for batch_number in range(num_batches):\n",
    "    print(\"last_sort\", last_sort)\n",
    "\n",
    "    for data, raw_data  in extract_data_from_index(es, index_pre, search_query, last_sort):\n",
    "        last_sort = raw_data['sort']\n",
    "        print(\"document_number --> \", index)\n",
    "        print(\"URL:\", data['url'])\n",
    "\n",
    "        if should_be_ignored(data):\n",
    "            #print(\"Filtering - Ignore this page: -- \", data['url'])\n",
    "            #print(\"nothing\")\n",
    "            ignored_page += 1\n",
    "        else:\n",
    "            if data['language'] == \"en\":\n",
    "                nlp = english_nlp\n",
    "            else:\n",
    "                nlp = german_nlp\n",
    "\n",
    "            # Extract people\n",
    "            people = list(extract_people_names(data['content'], nlp))\n",
    "\n",
    "            research_data = {\n",
    "                'id': data['extracted_from'] + '_' + raw_data['_id'],\n",
    "                'url': data['url'],\n",
    "                'title': data['title'],\n",
    "                'language': data['language'],\n",
    "                'type': 'research projects',\n",
    "                'last_update': datetime.utcnow().isoformat(timespec='milliseconds') + \"Z\",\n",
    "                'extracted_from': data['extracted_from'],\n",
    "                'people': people,\n",
    "                #'summary': summarize(data['content'])\n",
    "            }\n",
    "            considered_page += 1\n",
    "            #print(\"Filtering - consider this page: --  -- \", data['url'])\n",
    "            #print(\"Summary:\", research_data['summary'])\n",
    "            #es.index(index=research_project_index, body=research_data)\n",
    "        index += 1\n",
    "        if index == limit:\n",
    "            break_outer_loop = True\n",
    "            break\n",
    "    if break_outer_loop:\n",
    "        break\n",
    "\n",
    "print(\"IGNORED:\", ignored_page)\n",
    "print(\"CONSIDERED:\", considered_page)\n",
    "\n",
    "\"\"\"\n",
    "Prüfung von 100 Seiten (100 ersten Preprocessing Dokumenten nach Timestamp asc)\n",
    "\n",
    "=== Script Ergebnisse ===\n",
    "Keine Projektseiten: 57\n",
    "Projektseiten: 43\n",
    "\n",
    "\n",
    "=== Meine Ergebnisse ===\n",
    "Keine Projektseiten: 63\n",
    "Projektseiten: 37\n",
    "\n",
    "Es gibt einige Seiten, die einen Titel und eine Url haben, die dem Titel oder URL eines Projekts entsprechen, aber entweder sind sie oft leer, enthalten keinen Projekttext oder sind nur eine Liste von mehreren Projekten, die ich als Mensch nicht als Projektseiten betrachte. Das erklärt den Unterschied.\n",
    "\n",
    "Es ist möglich, dass wir auch mehrere positive False in größeren Stichproben haben. Es gibt noch einige Verbesserungen, die wir vornehmen müssen.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
